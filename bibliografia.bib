@article{Nodelman2002,
    archivePrefix = {arXiv},
    arxivId = {http://arxiv.org/abs/1212.2498},
    author = {Nodelman, Uri and Shelton, CR and Koller, Daphne},
    eprint = {/arxiv.org/abs/1212.2498},
    file = {:home/leodido/Documenti/Papers/Proceedings of the Nineteenth \ldots/2002 - Learning continuous time Bayesian networks - Nodelman, Shelton, Koller.pdf:pdf},
    journal = {Proceedings of the Nineteenth \ldots},
    number = {X},
    primaryClass = {http:},
    title = {{Learning continuous time Bayesian networks}},
    url = {http://dl.acm.org/citation.cfm?id=2100639},
    year = {2002}
}
@phdthesis{Nodelman2007,
    author = {Nodelman, Uri D.},
    file = {:home/leodido/Documenti/Papers/Unknown/2007 - Continuos Time Bayesian Networks - Nodelman.pdf:pdf},
    number = {June},
    school = {Stanford University},
    title = {{Continuos Time Bayesian Networks}},
    year = {2007}
}
@article{Stella2012,
    abstract = {The class of continuous time Bayesian network classifiers is defined; it solves the problem of supervised classification on multivariate trajectories evolving in continuous time. The trajectory consists of the values of discrete attributes that are measured in continuous time, while the predicted class is expected to occur in the future. Two instances from this class, namely the continuous time naive Bayes classifier and the continuous time tree augmented naive Bayes classifier, are introduced and analyzed. They implement a trade-off between computational complexity and classification accuracy. Learning and inference for the class of continuous time Bayesian network classifiers are addressed, in the case where complete data are available. A learning algorithm for the continuous time naive Bayes classifier and an exact inference algorithm for the class of continuous time Bayesian network classifiers are described. The performance of the continuous time naive Bayes classifier is assessed in the case where real-time feedback to neurological patients undergoing motor rehabilitation must be provided.},
    author = {Stella, F and Amer, Y},
    doi = {10.1016/j.jbi.2012.07.002},
    file = {:home/leodido/Documenti/Papers/Journal of biomedical informatics/2012 - Continuous time Bayesian network classifiers. - Stella, Amer.pdf:pdf},
    issn = {1532-0480},
    journal = {Journal of biomedical informatics},
    keywords = {bayesian classifiers},
    month = dec,
    number = {6},
    pages = {1108--19},
    pmid = {22846170},
    publisher = {Elsevier Inc.},
    title = {{Continuous time Bayesian network classifiers.}},
    url = {http://www.ncbi.nlm.nih.gov/pubmed/22846170},
    volume = {45},
    year = {2012}
}
@book{Russel2003,
    author = {Russell, Stuart J. and Norvig, Peter},
    isbn = {0137903952},
    keywords = {ai, books},
    publisher = {Pearson Education},
    title = {Artificial Intelligence: A Modern Approach},
    url = {http://portal.acm.org/citation.cfm?id=773294},
    year = {2003}
}
@article{Heckerman1995,
    author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
    doi = {10.1007/BF00994016},
    file = {:home/leodido/Documenti/Papers/Machine Learning/1995 - Learning Bayesian networks The combination of knowledge and statistical data - Heckerman, Geiger, Chickering.pdf:pdf},
    issn = {0885-6125},
    journal = {Machine Learning},
    month = sep,
    number = {3},
    pages = {197--243},
    title = {{Learning Bayesian networks: The combination of knowledge and statistical data}},
    url = {http://link.springer.com/10.1007/BF00994016},
    volume = {20},
    year = {1995}
}
@article{Dempster1977,
    abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
    author = {Dempster, A P and Laird, N M and Rubin, D B},
    doi = {10.2307/2984875},
    file = {:home/leodido/Documenti/Papers/Journal of the Royal Statistical Society Series B Methodological/1977 - Maximum likelihood from incomplete data via the EM algorithm - Dempster, Laird, Rubin.pdf:pdf},
    isbn = {0000000779},
    issn = {00359246},
    journal = {Journal of the Royal Statistical Society Series B Methodological},
    number = {1},
    pages = {1--38},
    pmid = {9501024},
    publisher = {JSTOR},
    series = {Series B},
    title = {{Maximum likelihood from incomplete data via the EM algorithm}},
    url = {http://www.jstor.org/stable/2984875},
    volume = {39},
    year = {1977}
}
@book{Norris1998,
  author    = {James R. Norris},
  title     = {Markov chains},
  publisher = {Cambridge University Press},
  series    = {Cambridge series in statistical and probabilistic mathematics},
  year      = {1998},
  isbn      = {978-0-521-48181-6},
  pages     = {I-XVI, 1-237},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
@book{Korb2011,
  title={Bayesian Artificial Intelligence},
  author={Korb, K.B. and Nicholson, A.E.},
  isbn={9781439815915},
  lccn={2010043669},
  series={Chapman \& Hall / CRC Computer Science and Data Analysis},
  year={2011},
  publisher={CRC PressINC}
}
