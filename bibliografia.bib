@article{Nodelman2002,
    archivePrefix = {arXiv},
    arxivId = {http://arxiv.org/abs/1212.2498},
    author = {Nodelman, Uri and Shelton, CR and Koller, Daphne},
    eprint = {/arxiv.org/abs/1212.2498},
    journal = {Proceedings of the Nineteenth \ldots},
    number = {X},
    primaryClass = {http:},
    title = {{Learning continuous time Bayesian networks}},
    url = {http://dl.acm.org/citation.cfm?id=2100639},
    year = {2002}
}
@phdthesis{Nodelman2007,
    author = {Nodelman, Uri D.},
    file = {:home/leodido/Documenti/Papers/Unknown/2007 - Continuos Time Bayesian Networks - Nodelman.pdf:pdf},
    number = {June},
    school = {Stanford University},
    title = {{Continuos Time Bayesian Networks}},
    year = {2007}
}
@article{Stella2012,
    abstract = {The class of continuous time Bayesian network classifiers is defined; it solves the problem of supervised classification on multivariate trajectories evolving in continuous time. The trajectory consists of the values of discrete attributes that are measured in continuous time, while the predicted class is expected to occur in the future. Two instances from this class, namely the continuous time naive Bayes classifier and the continuous time tree augmented naive Bayes classifier, are introduced and analyzed. They implement a trade-off between computational complexity and classification accuracy. Learning and inference for the class of continuous time Bayesian network classifiers are addressed, in the case where complete data are available. A learning algorithm for the continuous time naive Bayes classifier and an exact inference algorithm for the class of continuous time Bayesian network classifiers are described. The performance of the continuous time naive Bayes classifier is assessed in the case where real-time feedback to neurological patients undergoing motor rehabilitation must be provided.},
    author = {Stella, F and Amer, Y},
    doi = {10.1016/j.jbi.2012.07.002},
    issn = {1532-0480},
    journal = {Journal of biomedical informatics},
    keywords = {bayesian classifiers},
    month = dec,
    number = {6},
    pages = {1108--19},
    pmid = {22846170},
    publisher = {Elsevier Inc.},
    title = {{Continuous time Bayesian network classifiers.}},
    url = {http://www.ncbi.nlm.nih.gov/pubmed/22846170},
    volume = {45},
    year = {2012}
}
@techreport{Chickering1994,
    author = {Chickering, David Maxwell},
    title = {{Learning Bayesian networks is NP-hard}},
    institution = {Microsoft Research},
    year = {1994}
}
@book{Russel2003,
    author = {Russell, Stuart J. and Norvig, Peter},
    isbn = {0137903952},
    keywords = {ai, books},
    publisher = {Pearson Education},
    title = {Artificial Intelligence: A Modern Approach},
    url = {http://portal.acm.org/citation.cfm?id=773294},
    year = {2003}
}
@book{Pearl1988,
    author = {Pearl, Judea},
    title = {Probabilistic reasoning in intelligent systems: networks of plausible inference},
    year = {1988},
    isbn = {0-934613-73-7},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA},
}
@inproceedings{Verma1991,
    address = {North Holland},
    author = {Verma, Thomas S. and Pearl, Judea},
    booktitle = {Uncertainty in Artificial Intelligence},
    pages = {255--268},
    title = {{Equivalence and synthesis of causal models}},
    year = {1991}
}
@article{Chickering2013,
    abstract = {We present a simple characterization of equivalent Bayesian network structures based on local transformations. The significance of the characterization is twofold. First, we are able to easily prove several new invariant properties of theoretical interest for equivalent structures. Second, we use the characterization to derive an efficient algorithm that identifies all of the compelled edges in a structure. Compelled edge identification is of particular importance for learning Bayesian network structures from data because these edges indicate causal relationships when certain assumptions hold.},
    author = {Chickering, David Maxwell},
    pages = {87--98},
    journal = {CoRR},
    title = {{A Transformational Characterization of Equivalent Bayesian Network Structures}},
    url = {http://arxiv.org/abs/1302.4938},
    year = {2013}
}
@book{Loeve1978,
    author = {Lo{\`e}ve, Michel},
    title = {Probability theory. II Edition.},
    edition = {Fourth},
    note = {Graduate Texts in Mathematics, Vol. 46},
    publisher = {Springer-Verlag},
    address = {New York},
    year = {1978},
    pages = {xvi+413},
    isbn = {0-387-90262-7}
}
@book{DudaHart1973,
    address = {New Yotk},
    author = {Duda, R. O. and Hart, P. E.},
    publisher = {John Willey \& Sons},
    title = {Pattern Classification and Scene Analysis},
    year = 1973
}
@inproceedings{Langley1992,
    author    = {Pat Langley and Wayne Iba and Kevin Thompson},
    title     = {{An Analysis of Bayesian Classifiers}},
    booktitle = {AAAI},
    year      = {1992},
    isbn      = {0-262-51063-4},
    pages     = {223-228}
}
@article{Friedman1997,
    author = {Friedman, N and Geiger, D and Goldszmidt, M},
    journal = {Machine learning},
    keywords = {bayesian networks,classification},
    pages = {131--163},
    title = {{Bayesian network classifiers}},
    url = {http://link.springer.com/article/10.1023/A\%3A1007465528199},
    volume = {163},
    year = {1997}
}
@article{Heckerman1995,
    author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
    doi = {10.1007/BF00994016},
    issn = {0885-6125},
    journal = {Machine Learning},
    month = sep,
    number = {3},
    pages = {197--243},
    title = {{Learning Bayesian networks: The combination of knowledge and statistical data}},
    url = {http://link.springer.com/10.1007/BF00994016},
    volume = {20},
    year = {1995}
}
@article{Heckerman1996,
    abstract = {A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with Bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.},
    author = {Heckerman, David},
    chapter = {3},
    doi = {10.1007/978-3-540-85066-3},
    editor = {Holmes, Dawn E and Jain, Lakhmi C},
    institution = {Microsoft Research},
    isbn = {0262600323},
    issn = {1860949X},
    journal = {Innovations in Bayesian Networks},
    number = {November},
    pages = {33--82},
    pmid = {20108943},
    publisher = {Springer},
    series = {Studies in Computational Intelligence},
    title = {{A Tutorial on Learning With Bayesian Networks}},
    url = {http://www.springerlink.com/index/62mv333389016034.pdf},
    volume = {1995},
    year = {1996}
}
@inproceedings{Shachter1990,
 author = {Shachter, Ross D. and Peot, Mark A.},
 title = {Simulation Approaches to General Probabilistic Inference on Belief Networks},
 booktitle = {Proceedings of the Fifth Annual Conference on Uncertainty in Artificial Intelligence},
 series = {UAI '89},
 year = {1990},
 isbn = {0-444-88738-5},
 pages = {221--234},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=647232.719570},
 publisher = {North-Holland Publishing Co.},
 address = {Amsterdam, The Netherlands}
}
@article{Geman1984,
 author = {Geman, Stuart and Geman, Donald},
 title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 volume = {6},
 number = {6},
 year = {1984},
 issn = {0162-8828},
 pages = {721--741},
 numpages = {21},
 url = {http://dx.doi.org/10.1109/TPAMI.1984.4767596},
 doi = {10.1109/TPAMI.1984.4767596},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Annealing, Gibbs distribution, MAP estimate, Markov random field, image restoration, line process, relaxation, scene modeling, spatial degradation}
}
@article{Gilks1996,
  title = {Markov chain Monte Carlo in practice},
  author = {Gilks, WR and Richardson, S and Spiegelhalter, DJ},
  year = {1996},
  publisher = {Chapman and Hall}
}
@inproceedings{MacKay1998,
 author = {MacKay, D. J. C.},
 title = {Introduction to Monte Carlo methods},
 booktitle = {Proceedings of the NATO Advanced Study Institute on Learning in graphical models},
 year = {1998},
 pages = {175--204},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=299068.299077},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA}
}
@article{Dempster1977,
    abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
    author = {Dempster, A P and Laird, N M and Rubin, D B},
    doi = {10.2307/2984875},
    file = {:home/leodido/Documenti/Papers/Journal of the Royal Statistical Society Series B Methodological/1977 - Maximum likelihood from incomplete data via the EM algorithm - Dempster, Laird, Rubin.pdf:pdf},
    isbn = {0000000779},
    issn = {00359246},
    journal = {Journal of the Royal Statistical Society Series B Methodological},
    number = {1},
    pages = {1--38},
    pmid = {9501024},
    publisher = {JSTOR},
    series = {Series B},
    title = {{Maximum likelihood from incomplete data via the EM algorithm}},
    url = {http://www.jstor.org/stable/2984875},
    volume = {39},
    year = {1977}
}
@book{Norris1998,
  author    = {James R. Norris},
  title     = {Markov chains},
  publisher = {Cambridge University Press},
  series    = {Cambridge series in statistical and probabilistic mathematics},
  year      = {1998},
  isbn      = {978-0-521-48181-6},
  pages     = {I-XVI, 1-237},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
@book{Korb2011,
  title={Bayesian Artificial Intelligence},
  author={Korb, K.B. and Nicholson, A.E.},
  isbn={9781439815915},
  lccn={2010043669},
  series={Chapman \& Hall / CRC Computer Science and Data Analysis},
  year={2011},
  publisher={CRC PressINC}
}
